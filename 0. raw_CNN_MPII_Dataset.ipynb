{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar imágenes que están en el csv y las que no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "names = data['NAME'].unique()\n",
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "count = 0\n",
    "for name in names:\n",
    "    source_path = images_path + \"\\\\\" + name\n",
    "    category = np.array(data.loc[data['NAME'] == name, 'Category'])[0]\n",
    "    destination_path = images_path + \"\\\\\" + \"in_csv\" + \"\\\\\" + name\n",
    "    \n",
    "    if os.path.isfile(source_path):\n",
    "        os.rename(source_path, destination_path)\n",
    "    count += 1\n",
    "    if count % 250 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(count)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>015601864.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>015599452.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005808361.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>086617615.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060111501.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17367</th>\n",
       "      <td>033474347.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17368</th>\n",
       "      <td>082650067.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17369</th>\n",
       "      <td>072772110.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17370</th>\n",
       "      <td>039361034.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17371</th>\n",
       "      <td>084761779.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17372 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                NAME     Activity        Category\n",
       "0      015601864.jpg      curling          sports\n",
       "1      015599452.jpg      curling          sports\n",
       "2      005808361.jpg      curling          sports\n",
       "3      086617615.jpg      curling          sports\n",
       "4      060111501.jpg      curling          sports\n",
       "...              ...          ...             ...\n",
       "17367  033474347.jpg  pushing car  transportation\n",
       "17368  082650067.jpg  pushing car  transportation\n",
       "17369  072772110.jpg  pushing car  transportation\n",
       "17370  039361034.jpg  pushing car  transportation\n",
       "17371  084761779.jpg  pushing car  transportation\n",
       "\n",
       "[17372 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mpii_dataset_clean_v2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'inactivity quiet-light', 'miscellaneous', 'occupation',\n",
       "       'water activities', 'home activities', 'lawn and garden',\n",
       "       'religious activities', 'winter activities',\n",
       "       'conditioning exercise', 'bicycling', 'fishing and hunting',\n",
       "       'dancing', 'walking', 'running', 'self care', 'home repair',\n",
       "       'music playing', 'transportation', 'volunteer activities'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(396,)\n",
      "(20,)\n",
      "['curling' 'sitting quietly'\n",
      " 'sitting, talking in person, on the phone, computer, or text messaging, light effort'\n",
      " 'truck driving, loading and unloading truck, tying down load, standing, walking and carrying heavy lo'\n",
      " 'swimming, synchronized' 'scrubbing floors'\n",
      " 'mopping, standing, light effort'\n",
      " 'chambermaid, hotel housekeeper, making bed, cleaning bathroom, pushing cart'\n",
      " 'cleaning, general'\n",
      " 'implied walkingstanding - picking up yard, light, picking flowers or vegetables'\n",
      " 'gardening, general, moderate effort' 'standing, talking in person'\n",
      " 'standing, miscellaneous' 'lawn bowling, bocce ball, outdoor'\n",
      " 'serving food in church' 'skiing, downhill' 'skiing, cross-country'\n",
      " 'skiing, climbing up'\n",
      " 'horse grooming, feeding, cleaning, harnessing and unharnessing'\n",
      " 'horse cart, driving, standing or sitting'\n",
      " 'video exercise workouts, TV conditioning programs' 'bicycling, general'\n",
      " 'hacky sack' 'shoveling snow, by hand' 'fishing, general' 'ironing'\n",
      " 'ballroom' 'therapeutic exercise ball, Fitball exercise'\n",
      " 'standing, arts and crafts, sand painting, carving, weaving'\n",
      " 'volleyball, indoor' 'rock climbing' 'gymnastics, general'\n",
      " 'basketball, game (Taylor Code 490)' 'basketball'\n",
      " 'hunting, bow and arrow, or crossbow' 'laying crushed rock'\n",
      " 'kayaking, moderate effort' 'swimming, sidestroke, general'\n",
      " 'marching, military, no pack' 'tubing, floating on a river, general'\n",
      " 'whitewater rafting, kayaking, or canoeing' 'bicycling, racing and road'\n",
      " 'bicycling, mountain' 'water aerobics, water calisthenics' 'dog sledding'\n",
      " 'backpacking' 'orienteering' 'tailoring' 'racquetball'\n",
      " 'playing with animals' 'operating snow blower, walking'\n",
      " 'digging, spading, filling garden, composting' 'stretching, mild'\n",
      " 'sitting, writing, desk work, typing' 'running' 'jogging'\n",
      " 'skindiving or SCUBA diving as a frogman, Navy Seal' 'eating, sitting'\n",
      " 'horseback riding' 'track and field' 'juggling' 'elder care'\n",
      " 'planting, potting, transplanting seedlings or plants, light effort'\n",
      " 'planting seedlings, shrub, stooping, moderate effort'\n",
      " 'irrigation channels, opening and closing ports' 'fishing, ice, sitting'\n",
      " 'tanning hides, general' 'painting, outside home (Taylor Code 650)'\n",
      " 'drawing, writing, painting, standing'\n",
      " 'playing musical instruments, general' 'bicycling, stationary'\n",
      " 'mowing lawn, walk, hand mower (Taylor Code 570)' 'yard work, general'\n",
      " 'walking, general' 'washing and waxing hull of sailboat or airplane'\n",
      " 'pushing car'\n",
      " 'moving, carrying or pushing heavy objects, 75 lbs or more, only active time (e.g., desks, moving van'\n",
      " 'paddle boat' 'calisthenics' 'piano, sitting'\n",
      " 'kneeling in church or at home, praying'\n",
      " 'police, making an arrest, standing' 'cooking or food preparation'\n",
      " 'farming, taking care of animals (e.g., grooming, brushing, shearing sheep, assisting with birthing,'\n",
      " 'plumbing activities' 'teaching exercise class' 'hiking'\n",
      " 'walking, for exerceise, with ski poles' 'playing with children'\n",
      " 'sitting, doing work' 'resistance training'\n",
      " 'carpentry, finishing or refinishing cabinets or furniture'\n",
      " 'painting inside house,wallpapering, scraping paint' 'croquet'\n",
      " 'using heavy tools' 'standing quietly, standing in a line'\n",
      " 'motor scooter, motorcycle' 'carpentry, general' 'yoga, Power'\n",
      " 'yoga, Nadisodhana' 'hockey, ice, competitive' 'billiards' 'horse racing'\n",
      " 'home exercise, general' 'hunting, birds'\n",
      " 'working in scene shop, theater actor, backstage employee'\n",
      " 'aerobic, general' 'Irish step dancing'\n",
      " 'children_s games, adults playing (e.g., hopscotch, 4-square, dodgeball, playground apparatus, t-ball'\n",
      " 'watering lawn or garden, standing or walking' 'planting trees'\n",
      " 'reclining' 'reading'\n",
      " 'sitting, studying, general, including reading andor writing, light effort'\n",
      " 'canoeing, on camping trip (Taylor Code 270)' 'canoeing, portaging'\n",
      " 'manager, property'\n",
      " 'canoeing, harvesting wild rice, knocking rice off the stalks'\n",
      " 'hunting, general' 'wash dishes' 'vacuuming, general, moderate effort'\n",
      " 'cleaning, sweeping floor or carpet'\n",
      " 'sweeping garage, sidewalk or outside of house' 'serving food and drinks'\n",
      " 'climbing hills' 'running, stairs, up' 'health club exerceise classes'\n",
      " 'walkrun play with children, moderate, only active periods' 'landry'\n",
      " 'boating, power'\n",
      " 'grooming, washing hands, shaving,brushing teeth, putting on make-up, sitting or standing'\n",
      " 'fishing from river bank'\n",
      " 'fishing in stream, in waders (Taylor Code 670)' 'rope skipping, general'\n",
      " 'racking lawn'\n",
      " 'gardening with heavy power tools, tilling a garden, chain saw'\n",
      " 'hunting, rabbit, squirrel, prairie chick, raccoon, small game (Taylor Code 690)'\n",
      " 'moto-cross, off-road motor sports, all-terrain vehicle, general'\n",
      " 'soccer' 'Alaska Native Games, Eskimo Olympics, general'\n",
      " 'caulking, except log cabin' 'sitting, playing an instrument at church'\n",
      " 'rowing, stationary'\n",
      " 'moving household items upstairs, carrying boxes or furniture'\n",
      " 'fishing, catching fish with hands' 'chopping wood'\n",
      " 'boxing, punching bag' 'frisbee'\n",
      " 'moving furniture, household items, carrying boxes'\n",
      " 'manual or unskilled labor' 'cook, chef' 'home repair, general'\n",
      " 'carpentry, general, workshop (Taylor Code 620)' 'circuit training'\n",
      " 'rollerblading' 'tennis, hitting balls, non-game play, moderate effort'\n",
      " 'bicycling, BMX' 'wrestling (one match = 5 minutes)' 'golf'\n",
      " 'clearing brush' 'mowing lawn, power mower'\n",
      " 'non-food shopping, with or without a cart, standing or walking'\n",
      " 'child care' 'jogging, on a mini-tramp' 'archery, non-hunting' 'jai alai'\n",
      " 'skiing, water or wakeboarding (Taylor Code 220)'\n",
      " 'hairstylist (e.g., plaiting hair, manicure, make-up artist)'\n",
      " 'fishing from boat or canoe, sitting' 'running, marathon'\n",
      " 'pushing or pulling stroller with child or walking with children, 2.5 to 3.1 mph'\n",
      " 'lying quietly, sleeping' 'standing, doing work' 'surfing'\n",
      " 'putting away groceries (e.g. carrying groceries, shopping without a grocery cart), carrying packages'\n",
      " 'Elliptical trainer, moderate effort' 'coal mining'\n",
      " 'orange grove work, picking fruit' 'sanding floors with a power sander'\n",
      " 'skateboarding' 'copying documents, standing'\n",
      " 'ethnic or cultural dancing (e.g., Greek, Middle Eastern, hula, salsa, merengue, bamba y plena, flame'\n",
      " 'ballet, modern, or jazz'\n",
      " 'general dancing (e.g., disco, folk, Irish step dancing, line dancing, polka, contra, country)'\n",
      " 'pilates, general' 'feeding household animals' 'windsurfing' 'unicycling'\n",
      " 'laying or removing carpet' 'roofing' 'handball' 'masonry, concrete'\n",
      " 'weight lifting, power lifting' 'slimnastics, jazzercise'\n",
      " 'skindiving, scuba diving, snorkeling' 'swimming, general'\n",
      " 'swimming, backstroke' 'dusting or polishing furniture, general'\n",
      " 'painting,house, furniture, moderate effort'\n",
      " 'wheelbarrow, pushing garden cart or wheelbarrow' 'cleaning gutters'\n",
      " 'bakery, general' 'trimming shrubs or trees' 'fishing, commercial'\n",
      " 'felling trees' 'repairing appliances' 'snowmobiling' 'bookbinding'\n",
      " 'printing, paper industry worker, standing' 'paddleball' 'hockey, field'\n",
      " 'swimming, lake, ocean, river (Taylor Codes 280, 295)'\n",
      " 'riding in a car or truck' 'forestry'\n",
      " 'canoeing, kayaking, rowing, competitive' 'running, cross country'\n",
      " 'hammering nails' 'woodwind, sitting'\n",
      " 'laying tile or linoleum,repairing appliances' 'ski jumping'\n",
      " 'snowboarding' 'skiing, cross-country, biathlon, skating technique'\n",
      " 'skiing, general'\n",
      " 'maple syrupingsugar bushing (including carrying buckets, carrying wood)'\n",
      " 'kitchen activity, general, (e.g., cooking, washing dishes, cleaning up), moderate effort'\n",
      " 'bowling' 'picking fruits-vegetables' 'skating, ice'\n",
      " 'sledding, tobogganing, bobsledding, luge (Taylor Code 370)'\n",
      " 'water jogging' 'broomball' 'football, competitive'\n",
      " 'football, touch, flag' 'put on and removal of tarp - sailboat'\n",
      " 'volleyball, beach, in sand' 'aerobic, step' 'using crutches'\n",
      " 'accordion, sitting' 'rugby, touch, non-competitive' 'tennis, doubles'\n",
      " 'sitting, arts and crafts,  carving wood, weaving, spinning wool'\n",
      " 'laughing, sitting' 'custodial work'\n",
      " 'pushing a wheelchair, non-occupational' 'horn, standing'\n",
      " 'sitting at a sporting event, spectator' 'card playing,sitting'\n",
      " 'martial arts, different types'\n",
      " 'standing, child care, only active periods'\n",
      " 'swimming, breaststroke, recreational' 'diving, springboard or platform'\n",
      " 'swimming, butterfly, general' 'bird watching, slow walk'\n",
      " 'guitar, rock and roll band, standing' 'machine tooling'\n",
      " 'scraping and painting sailboat or powerboat'\n",
      " 'breastfeeding, sitting or reclining'\n",
      " 'polishing floors, standing, walking slowly, using electric polishing machine'\n",
      " 'driving tractor' 'shoveling dirt or mud' 'knitting, sewing'\n",
      " 'massage therapist, standing' 'walking the dog'\n",
      " 'guitar, classical, folk, sitting' 'carrying, loading or stacking wood'\n",
      " 'fire fighter' 'water walking' 'native New Zealander physical activities'\n",
      " 'loading unloading a car, implied walking' 'descending stairs'\n",
      " 'walking, applying fertilizer or seeding a lawn, push applicator'\n",
      " 'farming, feeding small animals' 'conducting orchestra, standing'\n",
      " 'caulking, chinking log cabin'\n",
      " 'touringtravelingvacation involving walking'\n",
      " 'farming, driving tasks (e.g., driving tractor or harvester)'\n",
      " 'hang gliding' 'postal carrier, walking to deliver mail'\n",
      " 'farming, milking by hand, cleaning pails, moderate effort'\n",
      " 'washing and waxing car' 'boxing, in ring, general' 'boxing, sparring'\n",
      " 'running, on a track, team practice' 'sailing' 'sailing, in competition'\n",
      " 'fire fighter, hauling hoses on ground, carryinghoisting equipment, breaking down walls etc., wearin'\n",
      " 'fishing, dip net, setting net and retrieving fish, general' 'farming'\n",
      " 'kickball' 'riding in a bus or train'\n",
      " 'driving delivery truck, taxi, shuttle bus, school bus'\n",
      " 'airline flight attendant'\n",
      " 'garbage collector, walking, dumping bins into truck' 'automobile repair'\n",
      " 'farming, cultivating field' 'carpentry, home remodeling tasks'\n",
      " 'typing, electric, manual or computer'\n",
      " 'steel mill, moderate effort (e.g., fettling, forging, tipping molds)'\n",
      " 'organizing room'\n",
      " 'upper body exercise, stationary bicycle - Airdyne (arms only) 40 rpm, moderate'\n",
      " 'engineer (e.g., mechanical or electrical)' 'pulling rickshaw'\n",
      " 'hanging sheet rock inside house' 'police, general'\n",
      " 'double bass, standing' 'trapping game, general' 'squash'\n",
      " 'driving automobile or light truck' 'softball, general'\n",
      " 'skydiving, base jumping, bungee jumping' 'stair climbing, fast pace'\n",
      " 'food shopping with or without a grocery cart, standing or walking'\n",
      " 'weeding, cultivating garden'\n",
      " 'carpentry, outside house, building a fence'\n",
      " 'fishing, set net, setting net and retrieving fish, general'\n",
      " 'rodeo sports' 'marching band' 'snow shoeing'\n",
      " 'digging sandbox, shoveling sand' 'horseshoe pitching, quoits' 'fencing'\n",
      " 'race walking' 'active workstation, treadmill desk, walking'\n",
      " 'tai chi, qi gong, general' 'drums, sitting'\n",
      " 'fishing with a spear, standing' 'organ, sitting' 'tennis'\n",
      " 'skating, roller (Taylor Code 360)' 'shuffleboard'\n",
      " 'farming, rice, planting, grain milling activities' 'hairstyling'\n",
      " 'skating, ice dancing'\n",
      " 'touringtravelingvacation involving riding in a vehicle'\n",
      " 'cleaning windows, washing windows, general' 'paddle boarding, standing'\n",
      " 'sitting, playing traditional video game, computer game'\n",
      " 'farming, milking by machine, light effort' 'riding snow blower'\n",
      " 'gardening, using containers, older adults more than 60 years'\n",
      " 'sitting, in class, general, including note-taking or class discussion'\n",
      " 'trombone, standing' 'wallyball, general' 'polo, on horseback'\n",
      " 'water volleyball'\n",
      " 'drumming (e.g., bongo, conga, benbe), moderate, sitting'\n",
      " 'trampoline, competitive' 'chess game, sitting' 'laundry worker'\n",
      " 'sitting, teaching stretching or yoga, or light effort exercise class'\n",
      " 'Anishinaabe Jingle Dancing' 'pistol shooting or trap shooting, standing'\n",
      " 'wiring, tapping-splicing' 'cricket, batting, bowling, fielding'\n",
      " 'table tennis, ping pong (Taylor Code 410)' 'street skiing'\n",
      " 'trumpet, standing' 'trampoline, recreational'\n",
      " 'sitting meetings, light effort, general, andor with talking involved (e.g., eating at a business me'\n",
      " 'cutting and smoking fish, drying fish or meat'\n",
      " 'badminton, competitive (Taylor Code 450)' 'snorkeling (Taylor Code 310)'\n",
      " 'sitting tasks, light effort (e.g., office work, chemistry lab work, computer work, light assembly re'\n",
      " 'standing, talking in church' 'farming, feeding cattle, horses'\n",
      " 'violin, sitting' 'volleyball, non-competitive'\n",
      " 'skating, speed, competitive' 'jet skiing, driving, in water'\n",
      " 'hanging storm windows' 'windsurfing or kitesurfing, winter'\n",
      " 'hockey, ice, general' 'making bed, changing linens'\n",
      " 'raking roof with snow rake' 'sharpening tools' 'cello, sitting'\n",
      " 'construction, outside, remodeling, new structures (e.g., roof repair, miscellaneous)'\n",
      " 'coaching' 'ski machine, general' 'locksmith' 'carrying load, upstairs'\n",
      " 'taking medication, sitting or standing' 'sitting in church'\n",
      " 'washing fence, painting fence, moderate effort'\n",
      " 'police, directing traffic, standing' 'slide board exercise, general'\n",
      " 'lacrosse' 'carpentry, sawing hardwood' 'flute, sitting'\n",
      " 'pushing plane in and out of hangar'\n",
      " 'stair climbing, using or climbing up ladder (Taylor Code 030)'\n",
      " 'darts, wall or lawn' 'badminton, social singles and doubles, general'\n",
      " \"Caribbean dance (Abakua, Beguine, Bellair, Bongo, Brukin's, Caribbean Quadrills, Dinki Mini, Gere, G\"\n",
      " 'high ropes course, multiple elements' 'shoe repair, general']\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().values.any())\n",
    "print(data['Activity'].unique().shape)\n",
    "print(data['Category'].unique().shape)\n",
    "print(data['Activity'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescalar imágenes a 256x256 (por si acaso) YA HECHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "path = ('..\\Datasets\\mpii_human_pose\\human_pose_images')\n",
    "dirs = os.listdir(path)\n",
    "outpath  = (r'..\\Datasets\\mpii_human_pose\\resized_images' + '\\\\')\n",
    "count = 0\n",
    "for item in dirs:\n",
    "    count += 1\n",
    "    obj = path + '\\\\' + item\n",
    "    if os.path.isfile(obj):\n",
    "        im = Image.open(obj)\n",
    "        f, e = os.path.splitext(item)\n",
    "        imResize = im.resize((256, 256), Image.ANTIALIAS)\n",
    "        imResize.save(outpath + '\\\\' + f + '.jpeg', 'JPEG', quality=90)\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "            ''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar imágenes y datos para entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('Category', axis=1)\n",
    "y = data['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>069597421.jpg</td>\n",
       "      <td>sitting, talking in person, on the phone, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>073872315.jpg</td>\n",
       "      <td>standing, arts and crafts, sand painting, carv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15789</th>\n",
       "      <td>058772539.jpg</td>\n",
       "      <td>eating, sitting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>069963957.jpg</td>\n",
       "      <td>rowing, stationary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10744</th>\n",
       "      <td>061143424.jpg</td>\n",
       "      <td>bicycling, stationary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14696</th>\n",
       "      <td>006110708.jpg</td>\n",
       "      <td>ballroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>039090600.jpg</td>\n",
       "      <td>irrigation channels, opening and closing ports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11798</th>\n",
       "      <td>073432341.jpg</td>\n",
       "      <td>water aerobics, water calisthenics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>093874989.jpg</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>089556285.jpg</td>\n",
       "      <td>moving furniture, household items, carrying boxes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                NAME                                           Activity\n",
       "230    069597421.jpg  sitting, talking in person, on the phone, comp...\n",
       "2141   073872315.jpg  standing, arts and crafts, sand painting, carv...\n",
       "15789  058772539.jpg                                    eating, sitting\n",
       "2386   069963957.jpg                                 rowing, stationary\n",
       "10744  061143424.jpg                              bicycling, stationary\n",
       "...              ...                                                ...\n",
       "14696  006110708.jpg                                           ballroom\n",
       "1099   039090600.jpg     irrigation channels, opening and closing ports\n",
       "11798  073432341.jpg                 water aerobics, water calisthenics\n",
       "6637   093874989.jpg                                             soccer\n",
       "2575   089556285.jpg  moving furniture, household items, carrying boxes\n",
       "\n",
       "[13897 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, \n",
    "                                                 test_size = 0.20, random_state = 2)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repartir las imágenes en carpeta 'train' y carpeta 'test'. Esto solo se hace una vez puesto que el random_state = 2 hace que los datos siempre se repartan de igual manera entre train y test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "names_train = x_train['NAME'].unique()\n",
    "names_test = x_test['NAME'].unique()\n",
    "\n",
    "names = np.concatenate((names_train, names_test), axis=0)\n",
    "\n",
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "count = 0\n",
    "for name in names:\n",
    "    source_path = images_path + \"\\\\\" + name\n",
    "    if name in names_train:\n",
    "        folder = 'train'\n",
    "    elif name in names_test:\n",
    "        folder = 'test'\n",
    "        \n",
    "    destination_path = images_path + \"\\\\\" + folder + \"\\\\\" + name\n",
    "    \n",
    "    if os.path.isfile(source_path):\n",
    "        os.rename(source_path, destination_path)\n",
    "    count += 1\n",
    "    if count % 250 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(count)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repartir las imágenes en carpetas-categoría**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Repartir imágenes en carpetas-categorías\n",
    "\n",
    "names_train = x_train['NAME'].unique()\n",
    "names_test = x_test['NAME'].unique()\n",
    "\n",
    "names = np.concatenate((names_train, names_test), axis=0)\n",
    "\n",
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "train_dir = os.path.join(images_path, \"train\")\n",
    "test_dir =  os.path.join(images_path, \"test\")\n",
    "\n",
    "categories = np.array(data['Category'].unique())\n",
    "\n",
    "# Crear carpetas para categorías\n",
    "#for category in categories:\n",
    "#    os.mkdir(os.path.join(train_dir, category))\n",
    "#    os.mkdir(os.path.join(test_dir, category))\n",
    "\n",
    "count = 0\n",
    "for name in names:\n",
    "    category_image = np.array(data[data['NAME'] == name].Category)[0]\n",
    "\n",
    "    if name in names_train:\n",
    "        folder = 'train'\n",
    "    elif name in names_test:\n",
    "        folder = 'test'      \n",
    "        \n",
    "    source_path = os.path.join(images_path, folder, name)\n",
    "    destination_path = os.path.join(images_path, folder, category_image, name)\n",
    "\n",
    "    if os.path.isfile(source_path):\n",
    "        os.rename(source_path, destination_path)\n",
    "        \n",
    "    count += 1\n",
    "    if count % 250 == 0:\n",
    "        print(count)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Red Neuronal Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Desired size of the image 256x256 with 3 bytes color\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(20, activation='softmax')    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               21234176  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 21,249,524\n",
      "Trainable params: 21,249,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "train_dir = os.path.join(images_path, \"train\")\n",
    "test_dir =  os.path.join(images_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13897 images belonging to 20 classes.\n",
      "Found 3475 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255.0)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20,\n",
    "                                                    class_mode = 'categorical', \n",
    "                                                    target_size =(150,150))\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, batch_size = 20,\n",
    "                                                 class_mode = 'categorical', \n",
    "                                                  target_size = (150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "695/695 [==============================] - 783s 1s/step - loss: 2.3192 - accuracy: 0.3014 - val_loss: 2.0774 - val_accuracy: 0.3672\n",
      "Epoch 2/100\n",
      "695/695 [==============================] - 540s 777ms/step - loss: 1.4513 - accuracy: 0.5588 - val_loss: 1.8508 - val_accuracy: 0.4696\n",
      "Epoch 3/100\n",
      "695/695 [==============================] - 501s 721ms/step - loss: 0.5329 - accuracy: 0.8403 - val_loss: 2.2403 - val_accuracy: 0.4846\n",
      "Epoch 4/100\n",
      "695/695 [==============================] - 518s 745ms/step - loss: 0.1348 - accuracy: 0.9657 - val_loss: 2.9873 - val_accuracy: 0.4668\n",
      "Epoch 5/100\n",
      "695/695 [==============================] - 503s 723ms/step - loss: 0.0596 - accuracy: 0.9876 - val_loss: 3.0790 - val_accuracy: 0.4691\n",
      "Epoch 6/100\n",
      "695/695 [==============================] - 516s 743ms/step - loss: 0.0608 - accuracy: 0.9847 - val_loss: 3.7640 - val_accuracy: 0.4581\n",
      "Epoch 7/100\n",
      "695/695 [==============================] - 510s 734ms/step - loss: 0.0505 - accuracy: 0.9852 - val_loss: 3.5410 - val_accuracy: 0.4731\n",
      "Epoch 8/100\n",
      "695/695 [==============================] - 552s 794ms/step - loss: 0.0431 - accuracy: 0.9882 - val_loss: 4.2573 - val_accuracy: 0.4291\n",
      "Epoch 9/100\n",
      "695/695 [==============================] - 538s 774ms/step - loss: 0.0488 - accuracy: 0.9869 - val_loss: 3.8939 - val_accuracy: 0.4757\n",
      "Epoch 10/100\n",
      "695/695 [==============================] - 555s 798ms/step - loss: 0.0452 - accuracy: 0.9874 - val_loss: 4.5414 - val_accuracy: 0.4414\n",
      "Epoch 11/100\n",
      "695/695 [==============================] - 554s 797ms/step - loss: 0.0440 - accuracy: 0.9879 - val_loss: 4.3720 - val_accuracy: 0.4236\n",
      "Epoch 12/100\n",
      "695/695 [==============================] - 595s 856ms/step - loss: 0.0231 - accuracy: 0.9937 - val_loss: 5.0779 - val_accuracy: 0.4363\n",
      "Epoch 13/100\n",
      "695/695 [==============================] - 563s 810ms/step - loss: 0.0319 - accuracy: 0.9899 - val_loss: 5.1718 - val_accuracy: 0.4590\n",
      "Epoch 14/100\n",
      "695/695 [==============================] - 564s 812ms/step - loss: 0.0365 - accuracy: 0.9902 - val_loss: 5.4966 - val_accuracy: 0.4184\n",
      "Epoch 15/100\n",
      "695/695 [==============================] - 551s 792ms/step - loss: 0.0523 - accuracy: 0.9849 - val_loss: 5.0535 - val_accuracy: 0.4475\n",
      "Epoch 16/100\n",
      "695/695 [==============================] - 569s 819ms/step - loss: 0.0410 - accuracy: 0.9901 - val_loss: 5.1797 - val_accuracy: 0.4276\n",
      "Epoch 17/100\n",
      "695/695 [==============================] - 599s 862ms/step - loss: 0.0170 - accuracy: 0.9950 - val_loss: 5.7488 - val_accuracy: 0.4354\n",
      "Epoch 18/100\n",
      "695/695 [==============================] - 561s 806ms/step - loss: 0.0211 - accuracy: 0.9947 - val_loss: 5.4020 - val_accuracy: 0.4308\n",
      "Epoch 19/100\n",
      "695/695 [==============================] - 577s 830ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 5.9187 - val_accuracy: 0.4452\n",
      "Epoch 20/100\n",
      "695/695 [==============================] - 563s 810ms/step - loss: 0.0324 - accuracy: 0.9900 - val_loss: 5.8235 - val_accuracy: 0.4135\n",
      "Epoch 21/100\n",
      "695/695 [==============================] - 582s 838ms/step - loss: 0.0191 - accuracy: 0.9944 - val_loss: 6.2391 - val_accuracy: 0.4176\n",
      "Epoch 22/100\n",
      "695/695 [==============================] - 583s 839ms/step - loss: 0.0228 - accuracy: 0.9942 - val_loss: 6.4454 - val_accuracy: 0.4184\n",
      "Epoch 23/100\n",
      "695/695 [==============================] - 541s 778ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 6.1106 - val_accuracy: 0.4256\n",
      "Epoch 24/100\n",
      "695/695 [==============================] - 541s 779ms/step - loss: 0.0191 - accuracy: 0.9960 - val_loss: 6.1842 - val_accuracy: 0.4124\n",
      "Epoch 25/100\n",
      "695/695 [==============================] - 563s 811ms/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 6.9100 - val_accuracy: 0.4210\n",
      "Epoch 26/100\n",
      "695/695 [==============================] - 517s 744ms/step - loss: 0.0358 - accuracy: 0.9915 - val_loss: 6.9669 - val_accuracy: 0.4147\n",
      "Epoch 27/100\n",
      "695/695 [==============================] - 512s 737ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 6.6660 - val_accuracy: 0.4233\n",
      "Epoch 28/100\n",
      "695/695 [==============================] - 617s 888ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 6.5521 - val_accuracy: 0.4354\n",
      "Epoch 29/100\n",
      "695/695 [==============================] - 501s 721ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 6.4608 - val_accuracy: 0.4291\n",
      "Epoch 30/100\n",
      "695/695 [==============================] - 501s 720ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 6.3945 - val_accuracy: 0.4342\n",
      "Epoch 31/100\n",
      "695/695 [==============================] - 527s 758ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 6.0977 - val_accuracy: 0.4256\n",
      "Epoch 32/100\n",
      "695/695 [==============================] - 569s 819ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 6.9947 - val_accuracy: 0.4040\n",
      "Epoch 33/100\n",
      "695/695 [==============================] - 592s 852ms/step - loss: 0.0682 - accuracy: 0.9845 - val_loss: 7.3522 - val_accuracy: 0.4216\n",
      "Epoch 34/100\n",
      "695/695 [==============================] - 663s 954ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 6.9393 - val_accuracy: 0.4153\n",
      "Epoch 35/100\n",
      "695/695 [==============================] - 663s 954ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 6.8484 - val_accuracy: 0.4233\n",
      "Epoch 36/100\n",
      "695/695 [==============================] - 537s 773ms/step - loss: 0.0131 - accuracy: 0.9988 - val_loss: 6.7802 - val_accuracy: 0.4161\n",
      "Epoch 37/100\n",
      "695/695 [==============================] - 462s 665ms/step - loss: 0.0058 - accuracy: 0.9994 - val_loss: 6.4269 - val_accuracy: 0.4207\n",
      "Epoch 38/100\n",
      "695/695 [==============================] - 470s 676ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 5.8546 - val_accuracy: 0.4247\n",
      "Epoch 39/100\n",
      "695/695 [==============================] - 533s 767ms/step - loss: 8.7919e-04 - accuracy: 0.9997 - val_loss: 5.9587 - val_accuracy: 0.4213\n",
      "Epoch 40/100\n",
      "695/695 [==============================] - 559s 805ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 6.3897 - val_accuracy: 0.4233\n",
      "Epoch 41/100\n",
      "695/695 [==============================] - 508s 731ms/step - loss: 0.0194 - accuracy: 0.9953 - val_loss: 7.8429 - val_accuracy: 0.3675\n",
      "Epoch 42/100\n",
      "695/695 [==============================] - 593s 853ms/step - loss: 0.0470 - accuracy: 0.9894 - val_loss: 7.6445 - val_accuracy: 0.4052\n",
      "Epoch 43/100\n",
      "695/695 [==============================] - 508s 731ms/step - loss: 0.0233 - accuracy: 0.9940 - val_loss: 7.3645 - val_accuracy: 0.3882\n",
      "Epoch 44/100\n",
      "695/695 [==============================] - 583s 839ms/step - loss: 0.0090 - accuracy: 0.9978 - val_loss: 8.4950 - val_accuracy: 0.3954\n",
      "Epoch 45/100\n",
      "695/695 [==============================] - 521s 750ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 8.6084 - val_accuracy: 0.4060\n",
      "Epoch 46/100\n",
      "695/695 [==============================] - 541s 779ms/step - loss: 9.7304e-04 - accuracy: 0.9996 - val_loss: 7.9017 - val_accuracy: 0.4037\n",
      "Epoch 47/100\n",
      "695/695 [==============================] - 554s 798ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 8.3138 - val_accuracy: 0.4046\n",
      "Epoch 48/100\n",
      "695/695 [==============================] - 553s 795ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 7.5661 - val_accuracy: 0.3988\n",
      "Epoch 49/100\n",
      "695/695 [==============================] - 548s 788ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 8.1860 - val_accuracy: 0.4046\n",
      "Epoch 50/100\n",
      "695/695 [==============================] - 552s 795ms/step - loss: 0.0425 - accuracy: 0.9900 - val_loss: 9.3801 - val_accuracy: 0.3914\n",
      "Epoch 51/100\n",
      "695/695 [==============================] - 553s 796ms/step - loss: 0.0177 - accuracy: 0.9963 - val_loss: 9.3360 - val_accuracy: 0.3957\n",
      "Epoch 52/100\n",
      "695/695 [==============================] - 556s 800ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 8.2126 - val_accuracy: 0.4029\n",
      "Epoch 53/100\n",
      "695/695 [==============================] - 464s 668ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 8.7670 - val_accuracy: 0.3922\n",
      "Epoch 54/100\n",
      "695/695 [==============================] - 460s 662ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 8.3626 - val_accuracy: 0.3988\n",
      "Epoch 55/100\n",
      "695/695 [==============================] - 470s 676ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 9.2598 - val_accuracy: 0.3963\n",
      "Epoch 56/100\n",
      "695/695 [==============================] - 550s 792ms/step - loss: 0.0149 - accuracy: 0.9963 - val_loss: 10.3971 - val_accuracy: 0.3971\n",
      "Epoch 57/100\n",
      "695/695 [==============================] - 551s 793ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 9.1022 - val_accuracy: 0.3804\n",
      "Epoch 58/100\n",
      "695/695 [==============================] - 550s 792ms/step - loss: 0.0019 - accuracy: 0.9991 - val_loss: 8.5558 - val_accuracy: 0.3957\n",
      "Epoch 59/100\n",
      "695/695 [==============================] - 549s 790ms/step - loss: 5.7875e-04 - accuracy: 0.9996 - val_loss: 8.4863 - val_accuracy: 0.3991\n",
      "Epoch 60/100\n",
      "695/695 [==============================] - 485s 697ms/step - loss: 5.7281e-04 - accuracy: 0.9996 - val_loss: 8.4318 - val_accuracy: 0.3983\n",
      "Epoch 61/100\n",
      "695/695 [==============================] - 457s 658ms/step - loss: 4.9899e-04 - accuracy: 0.9996 - val_loss: 8.3446 - val_accuracy: 0.3994\n",
      "Epoch 62/100\n",
      "695/695 [==============================] - 466s 670ms/step - loss: 5.4189e-04 - accuracy: 0.9996 - val_loss: 8.1774 - val_accuracy: 0.4012\n",
      "Epoch 63/100\n",
      "695/695 [==============================] - 551s 792ms/step - loss: 5.9647e-04 - accuracy: 0.9996 - val_loss: 8.0478 - val_accuracy: 0.4009\n",
      "Epoch 64/100\n",
      "695/695 [==============================] - 559s 805ms/step - loss: 8.0371e-04 - accuracy: 0.9996 - val_loss: 8.0827 - val_accuracy: 0.3882\n",
      "Epoch 65/100\n",
      "695/695 [==============================] - 519s 746ms/step - loss: 0.0177 - accuracy: 0.9970 - val_loss: 10.0860 - val_accuracy: 0.3764\n",
      "Epoch 66/100\n",
      "695/695 [==============================] - 562s 809ms/step - loss: 0.0349 - accuracy: 0.9927 - val_loss: 9.6498 - val_accuracy: 0.4043\n",
      "Epoch 67/100\n",
      "695/695 [==============================] - 552s 795ms/step - loss: 0.0084 - accuracy: 0.9984 - val_loss: 9.5593 - val_accuracy: 0.3968\n",
      "Epoch 68/100\n",
      "695/695 [==============================] - 528s 759ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 9.9842 - val_accuracy: 0.3977\n",
      "Epoch 69/100\n",
      "695/695 [==============================] - 547s 787ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 11.0154 - val_accuracy: 0.3965\n",
      "Epoch 70/100\n",
      "695/695 [==============================] - 529s 761ms/step - loss: 0.0091 - accuracy: 0.9968 - val_loss: 10.7551 - val_accuracy: 0.3882\n",
      "Epoch 71/100\n",
      "695/695 [==============================] - 512s 737ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 10.7728 - val_accuracy: 0.4012\n",
      "Epoch 72/100\n",
      "695/695 [==============================] - 585s 842ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 10.3496 - val_accuracy: 0.4072\n",
      "Epoch 73/100\n",
      "695/695 [==============================] - 579s 834ms/step - loss: 4.7992e-04 - accuracy: 0.9998 - val_loss: 10.1647 - val_accuracy: 0.4055\n",
      "Epoch 74/100\n",
      "695/695 [==============================] - 587s 845ms/step - loss: 5.6105e-04 - accuracy: 0.9998 - val_loss: 9.6667 - val_accuracy: 0.4046\n",
      "Epoch 75/100\n",
      "695/695 [==============================] - 582s 837ms/step - loss: 4.6472e-04 - accuracy: 0.9997 - val_loss: 9.7993 - val_accuracy: 0.4055\n",
      "Epoch 76/100\n",
      "695/695 [==============================] - 589s 847ms/step - loss: 4.5366e-04 - accuracy: 0.9998 - val_loss: 9.7059 - val_accuracy: 0.4058\n",
      "Epoch 77/100\n",
      "695/695 [==============================] - 587s 844ms/step - loss: 5.4792e-04 - accuracy: 0.9996 - val_loss: 9.4856 - val_accuracy: 0.4006\n",
      "Epoch 78/100\n",
      "695/695 [==============================] - 591s 851ms/step - loss: 5.1542e-04 - accuracy: 0.9996 - val_loss: 9.6475 - val_accuracy: 0.4023\n",
      "Epoch 79/100\n",
      "695/695 [==============================] - 511s 735ms/step - loss: 4.3178e-04 - accuracy: 0.9996 - val_loss: 9.2441 - val_accuracy: 0.3991\n",
      "Epoch 80/100\n",
      "695/695 [==============================] - 570s 821ms/step - loss: 0.0390 - accuracy: 0.9944 - val_loss: 14.8359 - val_accuracy: 0.3911\n",
      "Epoch 81/100\n",
      "695/695 [==============================] - 551s 793ms/step - loss: 0.0329 - accuracy: 0.9943 - val_loss: 11.6481 - val_accuracy: 0.3813\n",
      "Epoch 82/100\n",
      "695/695 [==============================] - 558s 802ms/step - loss: 0.0112 - accuracy: 0.9978 - val_loss: 10.9552 - val_accuracy: 0.3917\n",
      "Epoch 83/100\n",
      "695/695 [==============================] - 556s 800ms/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 13.1436 - val_accuracy: 0.3902\n",
      "Epoch 84/100\n",
      "695/695 [==============================] - 535s 770ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 11.6922 - val_accuracy: 0.4000\n",
      "Epoch 85/100\n",
      "695/695 [==============================] - 494s 711ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 11.3567 - val_accuracy: 0.4020\n",
      "Epoch 86/100\n",
      "695/695 [==============================] - 555s 798ms/step - loss: 5.7264e-04 - accuracy: 0.9998 - val_loss: 11.0744 - val_accuracy: 0.4009\n",
      "Epoch 87/100\n",
      "695/695 [==============================] - 557s 801ms/step - loss: 8.6790e-04 - accuracy: 0.9996 - val_loss: 10.5674 - val_accuracy: 0.4003\n",
      "Epoch 88/100\n",
      "695/695 [==============================] - 558s 803ms/step - loss: 5.4915e-04 - accuracy: 0.9997 - val_loss: 11.1501 - val_accuracy: 0.4006\n",
      "Epoch 89/100\n",
      "695/695 [==============================] - 555s 799ms/step - loss: 9.4461e-04 - accuracy: 0.9995 - val_loss: 10.9929 - val_accuracy: 0.3951\n",
      "Epoch 90/100\n",
      "695/695 [==============================] - 554s 798ms/step - loss: 0.0385 - accuracy: 0.9943 - val_loss: 12.5961 - val_accuracy: 0.3709\n",
      "Epoch 91/100\n",
      "695/695 [==============================] - 569s 819ms/step - loss: 0.0163 - accuracy: 0.9968 - val_loss: 12.9541 - val_accuracy: 0.3934\n",
      "Epoch 92/100\n",
      "695/695 [==============================] - 561s 808ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 11.4203 - val_accuracy: 0.3991\n",
      "Epoch 93/100\n",
      "695/695 [==============================] - 455s 655ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 11.9084 - val_accuracy: 0.3957\n",
      "Epoch 94/100\n",
      "695/695 [==============================] - 452s 651ms/step - loss: 6.6454e-04 - accuracy: 0.9996 - val_loss: 11.5228 - val_accuracy: 0.3991\n",
      "Epoch 95/100\n",
      "695/695 [==============================] - 454s 654ms/step - loss: 4.9489e-04 - accuracy: 0.9997 - val_loss: 11.3180 - val_accuracy: 0.3997\n",
      "Epoch 96/100\n",
      "695/695 [==============================] - 452s 651ms/step - loss: 4.2411e-04 - accuracy: 0.9997 - val_loss: 11.0142 - val_accuracy: 0.3994\n",
      "Epoch 97/100\n",
      "695/695 [==============================] - 476s 685ms/step - loss: 4.6715e-04 - accuracy: 0.9998 - val_loss: 10.9607 - val_accuracy: 0.3977\n",
      "Epoch 98/100\n",
      "695/695 [==============================] - 489s 703ms/step - loss: 4.5594e-04 - accuracy: 0.9997 - val_loss: 10.7198 - val_accuracy: 0.3960\n",
      "Epoch 99/100\n",
      "695/695 [==============================] - 471s 678ms/step - loss: 4.9755e-04 - accuracy: 0.9997 - val_loss: 10.8775 - val_accuracy: 0.3991\n",
      "Epoch 100/100\n",
      "695/695 [==============================] - 451s 649ms/step - loss: 0.0267 - accuracy: 0.9952 - val_loss: 13.0264 - val_accuracy: 0.3773\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, validation_data = test_generator,\n",
    "                    #steps_per_epoch = 1200, \n",
    "                    epochs = 100, # epochs a 100\n",
    "                    #validation_steps = 50, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(r\"saved\\raw_CNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_file = tf.keras.models.load_model('saved/primera_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/174 [=====================>........] - ETA: 23s - loss: 1.6322 - accuracy: 0.5060"
     ]
    }
   ],
   "source": [
    "model_from_file.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(100)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar modelo con imagen random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [None, 256, 3]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-caa6e4d560c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#imagenes = np.vstack([w])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3139\u001b[0m           expand_composites=True)\n\u001b[0;32m   3140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3141\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3142\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\navar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [None, 256, 3]\n"
     ]
    }
   ],
   "source": [
    "# Escribir email a Roberto para ver cómo pasar imágenes a la red para evaluarla\n",
    "'''from tensorflow.keras.preprocessing import image\n",
    "\n",
    "imgpath = 'fut1.jpg'\n",
    "imagen = image.load_img(imgpath, target_size=(256,256))\n",
    "\n",
    "w = image.img_to_array(imagen)\n",
    "w = np.expand_dims(x, axis=0)   # Mirar documentación de la función\n",
    "w = np.expand_dims(x, axis=0)\n",
    "imagenes = np.vstack([w])\n",
    "\n",
    "classes = model.predict(w, batch_size=20)\n",
    "\n",
    "print(classes[0])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
