{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import basic necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting images from .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "names = data['NAME'].unique()\n",
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "count = 0\n",
    "for name in names:\n",
    "    source_path = images_path + \"\\\\\" + name\n",
    "    category = np.array(data.loc[data['NAME'] == name, 'Category'])[0]\n",
    "    destination_path = images_path + \"\\\\\" + \"in_csv\" + \"\\\\\" + name\n",
    "    \n",
    "    if os.path.isfile(source_path):\n",
    "        os.rename(source_path, destination_path)\n",
    "    count += 1\n",
    "    if count % 250 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(count)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>015601864.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>015599452.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005808361.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>086617615.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060111501.jpg</td>\n",
       "      <td>curling</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17367</th>\n",
       "      <td>033474347.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17368</th>\n",
       "      <td>082650067.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17369</th>\n",
       "      <td>072772110.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17370</th>\n",
       "      <td>039361034.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17371</th>\n",
       "      <td>084761779.jpg</td>\n",
       "      <td>pushing car</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17372 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                NAME     Activity        Category\n",
       "0      015601864.jpg      curling          sports\n",
       "1      015599452.jpg      curling          sports\n",
       "2      005808361.jpg      curling          sports\n",
       "3      086617615.jpg      curling          sports\n",
       "4      060111501.jpg      curling          sports\n",
       "...              ...          ...             ...\n",
       "17367  033474347.jpg  pushing car  transportation\n",
       "17368  082650067.jpg  pushing car  transportation\n",
       "17369  072772110.jpg  pushing car  transportation\n",
       "17370  039361034.jpg  pushing car  transportation\n",
       "17371  084761779.jpg  pushing car  transportation\n",
       "\n",
       "[17372 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mpii_dataset_clean_v2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'inactivity quiet-light', 'miscellaneous', 'occupation',\n",
       "       'water activities', 'home activities', 'lawn and garden',\n",
       "       'religious activities', 'winter activities',\n",
       "       'conditioning exercise', 'bicycling', 'fishing and hunting',\n",
       "       'dancing', 'walking', 'running', 'self care', 'home repair',\n",
       "       'music playing', 'transportation', 'volunteer activities'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(data.isnull().values.any())\n",
    "print(data['Activity'].unique().shape)\n",
    "print(data['Category'].unique().shape)\n",
    "print(data['Activity'].unique())''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image rescaling script (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "path = ('..\\Datasets\\mpii_human_pose\\human_pose_images')\n",
    "dirs = os.listdir(path)\n",
    "outpath  = (r'..\\Datasets\\mpii_human_pose\\resized_images' + '\\\\')\n",
    "count = 0\n",
    "for item in dirs:\n",
    "    count += 1\n",
    "    obj = path + '\\\\' + item\n",
    "    if os.path.isfile(obj):\n",
    "        im = Image.open(obj)\n",
    "        f, e = os.path.splitext(item)\n",
    "        imResize = im.resize((256, 256), Image.ANTIALIAS)\n",
    "        imResize.save(outpath + '\\\\' + f + '.jpeg', 'JPEG', quality=90)\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "            ''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting images in corresponding folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('Category', axis=1)\n",
    "y = data['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>069597421.jpg</td>\n",
       "      <td>sitting, talking in person, on the phone, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>073872315.jpg</td>\n",
       "      <td>standing, arts and crafts, sand painting, carv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15789</th>\n",
       "      <td>058772539.jpg</td>\n",
       "      <td>eating, sitting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>069963957.jpg</td>\n",
       "      <td>rowing, stationary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10744</th>\n",
       "      <td>061143424.jpg</td>\n",
       "      <td>bicycling, stationary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14696</th>\n",
       "      <td>006110708.jpg</td>\n",
       "      <td>ballroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>039090600.jpg</td>\n",
       "      <td>irrigation channels, opening and closing ports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11798</th>\n",
       "      <td>073432341.jpg</td>\n",
       "      <td>water aerobics, water calisthenics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>093874989.jpg</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>089556285.jpg</td>\n",
       "      <td>moving furniture, household items, carrying boxes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                NAME                                           Activity\n",
       "230    069597421.jpg  sitting, talking in person, on the phone, comp...\n",
       "2141   073872315.jpg  standing, arts and crafts, sand painting, carv...\n",
       "15789  058772539.jpg                                    eating, sitting\n",
       "2386   069963957.jpg                                 rowing, stationary\n",
       "10744  061143424.jpg                              bicycling, stationary\n",
       "...              ...                                                ...\n",
       "14696  006110708.jpg                                           ballroom\n",
       "1099   039090600.jpg     irrigation channels, opening and closing ports\n",
       "11798  073432341.jpg                 water aerobics, water calisthenics\n",
       "6637   093874989.jpg                                             soccer\n",
       "2575   089556285.jpg  moving furniture, household items, carrying boxes\n",
       "\n",
       "[13897 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, \n",
    "                                                 test_size = 0.20, random_state = 2)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting in 'train' and 'test' folders. Only to be done once since seed value random_state=2 will always output the same distribution**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "names_train = x_train['NAME'].unique()\n",
    "names_test = x_test['NAME'].unique()\n",
    "\n",
    "names = np.concatenate((names_train, names_test), axis=0)\n",
    "\n",
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "count = 0\n",
    "for name in names:\n",
    "    source_path = images_path + \"\\\\\" + name\n",
    "    if name in names_train:\n",
    "        folder = 'train'\n",
    "    elif name in names_test:\n",
    "        folder = 'test'\n",
    "        \n",
    "    destination_path = images_path + \"\\\\\" + folder + \"\\\\\" + name\n",
    "    \n",
    "    if os.path.isfile(source_path):\n",
    "        os.rename(source_path, destination_path)\n",
    "    count += 1\n",
    "    if count % 250 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(count)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting images in the different (20) category folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Repartir imágenes en carpetas-categorías\n",
    "\n",
    "names_train = x_train['NAME'].unique()\n",
    "names_test = x_test['NAME'].unique()\n",
    "\n",
    "names = np.concatenate((names_train, names_test), axis=0)\n",
    "\n",
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "train_dir = os.path.join(images_path, \"train\")\n",
    "test_dir =  os.path.join(images_path, \"test\")\n",
    "\n",
    "categories = np.array(data['Category'].unique())\n",
    "\n",
    "# Crear carpetas para categorías\n",
    "#for category in categories:\n",
    "#    os.mkdir(os.path.join(train_dir, category))\n",
    "#    os.mkdir(os.path.join(test_dir, category))\n",
    "\n",
    "count = 0\n",
    "for name in names:\n",
    "    category_image = np.array(data[data['NAME'] == name].Category)[0]\n",
    "\n",
    "    if name in names_train:\n",
    "        folder = 'train'\n",
    "    elif name in names_test:\n",
    "        folder = 'test'      \n",
    "        \n",
    "    source_path = os.path.join(images_path, folder, name)\n",
    "    destination_path = os.path.join(images_path, folder, category_image, name)\n",
    "\n",
    "    if os.path.isfile(source_path):\n",
    "        os.rename(source_path, destination_path)\n",
    "        \n",
    "    count += 1\n",
    "    if count % 250 == 0:\n",
    "        print(count)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Desired size of the image 256x256 with 3 bytes color\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(20, activation='softmax')    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               21234176  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 21,249,524\n",
      "Trainable params: 21,249,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = (\"..\\Datasets\\mpii_human_pose\\human_pose_images\")\n",
    "train_dir = os.path.join(images_path, \"train\")\n",
    "test_dir =  os.path.join(images_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13897 images belonging to 20 classes.\n",
      "Found 3475 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255.0)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20,\n",
    "                                                    class_mode = 'categorical', \n",
    "                                                    target_size =(150,150))\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, batch_size = 20,\n",
    "                                                 class_mode = 'categorical', \n",
    "                                                  target_size = (150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "695/695 [==============================] - 783s 1s/step - loss: 2.3192 - accuracy: 0.3014 - val_loss: 2.0774 - val_accuracy: 0.3672\n",
      "Epoch 2/100\n",
      "695/695 [==============================] - 540s 777ms/step - loss: 1.4513 - accuracy: 0.5588 - val_loss: 1.8508 - val_accuracy: 0.4696\n",
      "Epoch 3/100\n",
      "695/695 [==============================] - 501s 721ms/step - loss: 0.5329 - accuracy: 0.8403 - val_loss: 2.2403 - val_accuracy: 0.4846\n",
      "Epoch 4/100\n",
      "695/695 [==============================] - 518s 745ms/step - loss: 0.1348 - accuracy: 0.9657 - val_loss: 2.9873 - val_accuracy: 0.4668\n",
      "Epoch 5/100\n",
      "695/695 [==============================] - 503s 723ms/step - loss: 0.0596 - accuracy: 0.9876 - val_loss: 3.0790 - val_accuracy: 0.4691\n",
      "Epoch 6/100\n",
      "695/695 [==============================] - 516s 743ms/step - loss: 0.0608 - accuracy: 0.9847 - val_loss: 3.7640 - val_accuracy: 0.4581\n",
      "Epoch 7/100\n",
      "695/695 [==============================] - 510s 734ms/step - loss: 0.0505 - accuracy: 0.9852 - val_loss: 3.5410 - val_accuracy: 0.4731\n",
      "Epoch 8/100\n",
      "695/695 [==============================] - 552s 794ms/step - loss: 0.0431 - accuracy: 0.9882 - val_loss: 4.2573 - val_accuracy: 0.4291\n",
      "Epoch 9/100\n",
      "695/695 [==============================] - 538s 774ms/step - loss: 0.0488 - accuracy: 0.9869 - val_loss: 3.8939 - val_accuracy: 0.4757\n",
      "Epoch 10/100\n",
      "695/695 [==============================] - 555s 798ms/step - loss: 0.0452 - accuracy: 0.9874 - val_loss: 4.5414 - val_accuracy: 0.4414\n",
      "Epoch 11/100\n",
      "695/695 [==============================] - 554s 797ms/step - loss: 0.0440 - accuracy: 0.9879 - val_loss: 4.3720 - val_accuracy: 0.4236\n",
      "Epoch 12/100\n",
      "695/695 [==============================] - 595s 856ms/step - loss: 0.0231 - accuracy: 0.9937 - val_loss: 5.0779 - val_accuracy: 0.4363\n",
      "Epoch 13/100\n",
      "695/695 [==============================] - 563s 810ms/step - loss: 0.0319 - accuracy: 0.9899 - val_loss: 5.1718 - val_accuracy: 0.4590\n",
      "Epoch 14/100\n",
      "695/695 [==============================] - 564s 812ms/step - loss: 0.0365 - accuracy: 0.9902 - val_loss: 5.4966 - val_accuracy: 0.4184\n",
      "Epoch 15/100\n",
      "695/695 [==============================] - 551s 792ms/step - loss: 0.0523 - accuracy: 0.9849 - val_loss: 5.0535 - val_accuracy: 0.4475\n",
      "Epoch 16/100\n",
      "695/695 [==============================] - 569s 819ms/step - loss: 0.0410 - accuracy: 0.9901 - val_loss: 5.1797 - val_accuracy: 0.4276\n",
      "Epoch 17/100\n",
      "695/695 [==============================] - 599s 862ms/step - loss: 0.0170 - accuracy: 0.9950 - val_loss: 5.7488 - val_accuracy: 0.4354\n",
      "Epoch 18/100\n",
      "695/695 [==============================] - 561s 806ms/step - loss: 0.0211 - accuracy: 0.9947 - val_loss: 5.4020 - val_accuracy: 0.4308\n",
      "Epoch 19/100\n",
      "695/695 [==============================] - 577s 830ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 5.9187 - val_accuracy: 0.4452\n",
      "Epoch 20/100\n",
      "695/695 [==============================] - 563s 810ms/step - loss: 0.0324 - accuracy: 0.9900 - val_loss: 5.8235 - val_accuracy: 0.4135\n",
      "Epoch 21/100\n",
      "695/695 [==============================] - 582s 838ms/step - loss: 0.0191 - accuracy: 0.9944 - val_loss: 6.2391 - val_accuracy: 0.4176\n",
      "Epoch 22/100\n",
      "695/695 [==============================] - 583s 839ms/step - loss: 0.0228 - accuracy: 0.9942 - val_loss: 6.4454 - val_accuracy: 0.4184\n",
      "Epoch 23/100\n",
      "695/695 [==============================] - 541s 778ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 6.1106 - val_accuracy: 0.4256\n",
      "Epoch 24/100\n",
      "695/695 [==============================] - 541s 779ms/step - loss: 0.0191 - accuracy: 0.9960 - val_loss: 6.1842 - val_accuracy: 0.4124\n",
      "Epoch 25/100\n",
      "695/695 [==============================] - 563s 811ms/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 6.9100 - val_accuracy: 0.4210\n",
      "Epoch 26/100\n",
      "695/695 [==============================] - 517s 744ms/step - loss: 0.0358 - accuracy: 0.9915 - val_loss: 6.9669 - val_accuracy: 0.4147\n",
      "Epoch 27/100\n",
      "695/695 [==============================] - 512s 737ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 6.6660 - val_accuracy: 0.4233\n",
      "Epoch 28/100\n",
      "695/695 [==============================] - 617s 888ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 6.5521 - val_accuracy: 0.4354\n",
      "Epoch 29/100\n",
      "695/695 [==============================] - 501s 721ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 6.4608 - val_accuracy: 0.4291\n",
      "Epoch 30/100\n",
      "695/695 [==============================] - 501s 720ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 6.3945 - val_accuracy: 0.4342\n",
      "Epoch 31/100\n",
      "695/695 [==============================] - 527s 758ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 6.0977 - val_accuracy: 0.4256\n",
      "Epoch 32/100\n",
      "695/695 [==============================] - 569s 819ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 6.9947 - val_accuracy: 0.4040\n",
      "Epoch 33/100\n",
      "695/695 [==============================] - 592s 852ms/step - loss: 0.0682 - accuracy: 0.9845 - val_loss: 7.3522 - val_accuracy: 0.4216\n",
      "Epoch 34/100\n",
      "695/695 [==============================] - 663s 954ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 6.9393 - val_accuracy: 0.4153\n",
      "Epoch 35/100\n",
      "695/695 [==============================] - 663s 954ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 6.8484 - val_accuracy: 0.4233\n",
      "Epoch 36/100\n",
      "695/695 [==============================] - 537s 773ms/step - loss: 0.0131 - accuracy: 0.9988 - val_loss: 6.7802 - val_accuracy: 0.4161\n",
      "Epoch 37/100\n",
      "695/695 [==============================] - 462s 665ms/step - loss: 0.0058 - accuracy: 0.9994 - val_loss: 6.4269 - val_accuracy: 0.4207\n",
      "Epoch 38/100\n",
      "695/695 [==============================] - 470s 676ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 5.8546 - val_accuracy: 0.4247\n",
      "Epoch 39/100\n",
      "695/695 [==============================] - 533s 767ms/step - loss: 8.7919e-04 - accuracy: 0.9997 - val_loss: 5.9587 - val_accuracy: 0.4213\n",
      "Epoch 40/100\n",
      "695/695 [==============================] - 559s 805ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 6.3897 - val_accuracy: 0.4233\n",
      "Epoch 41/100\n",
      "695/695 [==============================] - 508s 731ms/step - loss: 0.0194 - accuracy: 0.9953 - val_loss: 7.8429 - val_accuracy: 0.3675\n",
      "Epoch 42/100\n",
      "695/695 [==============================] - 593s 853ms/step - loss: 0.0470 - accuracy: 0.9894 - val_loss: 7.6445 - val_accuracy: 0.4052\n",
      "Epoch 43/100\n",
      "695/695 [==============================] - 508s 731ms/step - loss: 0.0233 - accuracy: 0.9940 - val_loss: 7.3645 - val_accuracy: 0.3882\n",
      "Epoch 44/100\n",
      "695/695 [==============================] - 583s 839ms/step - loss: 0.0090 - accuracy: 0.9978 - val_loss: 8.4950 - val_accuracy: 0.3954\n",
      "Epoch 45/100\n",
      "695/695 [==============================] - 521s 750ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 8.6084 - val_accuracy: 0.4060\n",
      "Epoch 46/100\n",
      "695/695 [==============================] - 541s 779ms/step - loss: 9.7304e-04 - accuracy: 0.9996 - val_loss: 7.9017 - val_accuracy: 0.4037\n",
      "Epoch 47/100\n",
      "695/695 [==============================] - 554s 798ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 8.3138 - val_accuracy: 0.4046\n",
      "Epoch 48/100\n",
      "695/695 [==============================] - 553s 795ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 7.5661 - val_accuracy: 0.3988\n",
      "Epoch 49/100\n",
      "695/695 [==============================] - 548s 788ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 8.1860 - val_accuracy: 0.4046\n",
      "Epoch 50/100\n",
      "695/695 [==============================] - 552s 795ms/step - loss: 0.0425 - accuracy: 0.9900 - val_loss: 9.3801 - val_accuracy: 0.3914\n",
      "Epoch 51/100\n",
      "695/695 [==============================] - 553s 796ms/step - loss: 0.0177 - accuracy: 0.9963 - val_loss: 9.3360 - val_accuracy: 0.3957\n",
      "Epoch 52/100\n",
      "695/695 [==============================] - 556s 800ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 8.2126 - val_accuracy: 0.4029\n",
      "Epoch 53/100\n",
      "695/695 [==============================] - 464s 668ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 8.7670 - val_accuracy: 0.3922\n",
      "Epoch 54/100\n",
      "695/695 [==============================] - 460s 662ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 8.3626 - val_accuracy: 0.3988\n",
      "Epoch 55/100\n",
      "695/695 [==============================] - 470s 676ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 9.2598 - val_accuracy: 0.3963\n",
      "Epoch 56/100\n",
      "695/695 [==============================] - 550s 792ms/step - loss: 0.0149 - accuracy: 0.9963 - val_loss: 10.3971 - val_accuracy: 0.3971\n",
      "Epoch 57/100\n",
      "695/695 [==============================] - 551s 793ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 9.1022 - val_accuracy: 0.3804\n",
      "Epoch 58/100\n",
      "695/695 [==============================] - 550s 792ms/step - loss: 0.0019 - accuracy: 0.9991 - val_loss: 8.5558 - val_accuracy: 0.3957\n",
      "Epoch 59/100\n",
      "695/695 [==============================] - 549s 790ms/step - loss: 5.7875e-04 - accuracy: 0.9996 - val_loss: 8.4863 - val_accuracy: 0.3991\n",
      "Epoch 60/100\n",
      "695/695 [==============================] - 485s 697ms/step - loss: 5.7281e-04 - accuracy: 0.9996 - val_loss: 8.4318 - val_accuracy: 0.3983\n",
      "Epoch 61/100\n",
      "695/695 [==============================] - 457s 658ms/step - loss: 4.9899e-04 - accuracy: 0.9996 - val_loss: 8.3446 - val_accuracy: 0.3994\n",
      "Epoch 62/100\n",
      "695/695 [==============================] - 466s 670ms/step - loss: 5.4189e-04 - accuracy: 0.9996 - val_loss: 8.1774 - val_accuracy: 0.4012\n",
      "Epoch 63/100\n",
      "695/695 [==============================] - 551s 792ms/step - loss: 5.9647e-04 - accuracy: 0.9996 - val_loss: 8.0478 - val_accuracy: 0.4009\n",
      "Epoch 64/100\n",
      "695/695 [==============================] - 559s 805ms/step - loss: 8.0371e-04 - accuracy: 0.9996 - val_loss: 8.0827 - val_accuracy: 0.3882\n",
      "Epoch 65/100\n",
      "695/695 [==============================] - 519s 746ms/step - loss: 0.0177 - accuracy: 0.9970 - val_loss: 10.0860 - val_accuracy: 0.3764\n",
      "Epoch 66/100\n",
      "695/695 [==============================] - 562s 809ms/step - loss: 0.0349 - accuracy: 0.9927 - val_loss: 9.6498 - val_accuracy: 0.4043\n",
      "Epoch 67/100\n",
      "695/695 [==============================] - 552s 795ms/step - loss: 0.0084 - accuracy: 0.9984 - val_loss: 9.5593 - val_accuracy: 0.3968\n",
      "Epoch 68/100\n",
      "695/695 [==============================] - 528s 759ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 9.9842 - val_accuracy: 0.3977\n",
      "Epoch 69/100\n",
      "695/695 [==============================] - 547s 787ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 11.0154 - val_accuracy: 0.3965\n",
      "Epoch 70/100\n",
      "695/695 [==============================] - 529s 761ms/step - loss: 0.0091 - accuracy: 0.9968 - val_loss: 10.7551 - val_accuracy: 0.3882\n",
      "Epoch 71/100\n",
      "695/695 [==============================] - 512s 737ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 10.7728 - val_accuracy: 0.4012\n",
      "Epoch 72/100\n",
      "695/695 [==============================] - 585s 842ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 10.3496 - val_accuracy: 0.4072\n",
      "Epoch 73/100\n",
      "695/695 [==============================] - 579s 834ms/step - loss: 4.7992e-04 - accuracy: 0.9998 - val_loss: 10.1647 - val_accuracy: 0.4055\n",
      "Epoch 74/100\n",
      "695/695 [==============================] - 587s 845ms/step - loss: 5.6105e-04 - accuracy: 0.9998 - val_loss: 9.6667 - val_accuracy: 0.4046\n",
      "Epoch 75/100\n",
      "695/695 [==============================] - 582s 837ms/step - loss: 4.6472e-04 - accuracy: 0.9997 - val_loss: 9.7993 - val_accuracy: 0.4055\n",
      "Epoch 76/100\n",
      "695/695 [==============================] - 589s 847ms/step - loss: 4.5366e-04 - accuracy: 0.9998 - val_loss: 9.7059 - val_accuracy: 0.4058\n",
      "Epoch 77/100\n",
      "695/695 [==============================] - 587s 844ms/step - loss: 5.4792e-04 - accuracy: 0.9996 - val_loss: 9.4856 - val_accuracy: 0.4006\n",
      "Epoch 78/100\n",
      "695/695 [==============================] - 591s 851ms/step - loss: 5.1542e-04 - accuracy: 0.9996 - val_loss: 9.6475 - val_accuracy: 0.4023\n",
      "Epoch 79/100\n",
      "695/695 [==============================] - 511s 735ms/step - loss: 4.3178e-04 - accuracy: 0.9996 - val_loss: 9.2441 - val_accuracy: 0.3991\n",
      "Epoch 80/100\n",
      "695/695 [==============================] - 570s 821ms/step - loss: 0.0390 - accuracy: 0.9944 - val_loss: 14.8359 - val_accuracy: 0.3911\n",
      "Epoch 81/100\n",
      "695/695 [==============================] - 551s 793ms/step - loss: 0.0329 - accuracy: 0.9943 - val_loss: 11.6481 - val_accuracy: 0.3813\n",
      "Epoch 82/100\n",
      "695/695 [==============================] - 558s 802ms/step - loss: 0.0112 - accuracy: 0.9978 - val_loss: 10.9552 - val_accuracy: 0.3917\n",
      "Epoch 83/100\n",
      "695/695 [==============================] - 556s 800ms/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 13.1436 - val_accuracy: 0.3902\n",
      "Epoch 84/100\n",
      "695/695 [==============================] - 535s 770ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 11.6922 - val_accuracy: 0.4000\n",
      "Epoch 85/100\n",
      "695/695 [==============================] - 494s 711ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 11.3567 - val_accuracy: 0.4020\n",
      "Epoch 86/100\n",
      "695/695 [==============================] - 555s 798ms/step - loss: 5.7264e-04 - accuracy: 0.9998 - val_loss: 11.0744 - val_accuracy: 0.4009\n",
      "Epoch 87/100\n",
      "695/695 [==============================] - 557s 801ms/step - loss: 8.6790e-04 - accuracy: 0.9996 - val_loss: 10.5674 - val_accuracy: 0.4003\n",
      "Epoch 88/100\n",
      "695/695 [==============================] - 558s 803ms/step - loss: 5.4915e-04 - accuracy: 0.9997 - val_loss: 11.1501 - val_accuracy: 0.4006\n",
      "Epoch 89/100\n",
      "695/695 [==============================] - 555s 799ms/step - loss: 9.4461e-04 - accuracy: 0.9995 - val_loss: 10.9929 - val_accuracy: 0.3951\n",
      "Epoch 90/100\n",
      "695/695 [==============================] - 554s 798ms/step - loss: 0.0385 - accuracy: 0.9943 - val_loss: 12.5961 - val_accuracy: 0.3709\n",
      "Epoch 91/100\n",
      "695/695 [==============================] - 569s 819ms/step - loss: 0.0163 - accuracy: 0.9968 - val_loss: 12.9541 - val_accuracy: 0.3934\n",
      "Epoch 92/100\n",
      "695/695 [==============================] - 561s 808ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 11.4203 - val_accuracy: 0.3991\n",
      "Epoch 93/100\n",
      "695/695 [==============================] - 455s 655ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 11.9084 - val_accuracy: 0.3957\n",
      "Epoch 94/100\n",
      "695/695 [==============================] - 452s 651ms/step - loss: 6.6454e-04 - accuracy: 0.9996 - val_loss: 11.5228 - val_accuracy: 0.3991\n",
      "Epoch 95/100\n",
      "695/695 [==============================] - 454s 654ms/step - loss: 4.9489e-04 - accuracy: 0.9997 - val_loss: 11.3180 - val_accuracy: 0.3997\n",
      "Epoch 96/100\n",
      "695/695 [==============================] - 452s 651ms/step - loss: 4.2411e-04 - accuracy: 0.9997 - val_loss: 11.0142 - val_accuracy: 0.3994\n",
      "Epoch 97/100\n",
      "695/695 [==============================] - 476s 685ms/step - loss: 4.6715e-04 - accuracy: 0.9998 - val_loss: 10.9607 - val_accuracy: 0.3977\n",
      "Epoch 98/100\n",
      "695/695 [==============================] - 489s 703ms/step - loss: 4.5594e-04 - accuracy: 0.9997 - val_loss: 10.7198 - val_accuracy: 0.3960\n",
      "Epoch 99/100\n",
      "695/695 [==============================] - 471s 678ms/step - loss: 4.9755e-04 - accuracy: 0.9997 - val_loss: 10.8775 - val_accuracy: 0.3991\n",
      "Epoch 100/100\n",
      "695/695 [==============================] - 451s 649ms/step - loss: 0.0267 - accuracy: 0.9952 - val_loss: 13.0264 - val_accuracy: 0.3773\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, validation_data = test_generator,\n",
    "                    #steps_per_epoch = 1200, \n",
    "                    epochs = 100, # epochs a 100\n",
    "                    #validation_steps = 50, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(r\"saved\\raw_CNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_file = tf.keras.models.load_model('saved/primera_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/174 [=====================>........] - ETA: 23s - loss: 1.6322 - accuracy: 0.5060"
     ]
    }
   ],
   "source": [
    "model_from_file.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(100)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft - Model evaluation with random image### Evaluar modelo con imagen random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.preprocessing import image\n",
    "\n",
    "imgpath = 'fut1.jpg'\n",
    "imagen = image.load_img(imgpath, target_size=(256,256))\n",
    "\n",
    "w = image.img_to_array(imagen)\n",
    "w = np.expand_dims(x, axis=0)   # Mirar documentación de la función\n",
    "w = np.expand_dims(x, axis=0)\n",
    "imagenes = np.vstack([w])\n",
    "\n",
    "classes = model.predict(w, batch_size=20)\n",
    "\n",
    "print(classes[0])''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
